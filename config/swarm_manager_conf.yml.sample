---
# Galaxy docker swarm manager configuration file
#
# To configure the location of this file, use the `swarm_manager_config_file`
# setting in galaxy.ini

# When the swarm manager daemonizes, it writes a pid file so that only one
# manager will run at a time. This is the path to that pid file.
# {xdg_data_home} will be templated automatically and defaults to
# ~/.local/share as per the XDG specification
#pid_file: '{xdg_data_home}/galaxy_swarm_manager.pid'

# Program output will be written to the log
#log_file: '{xdg_data_home}/galaxy_swarm_manager.log'

# As with GIE plugins, you can modify the base docker command ({docker_args}
# must be present and will be filled in with the docker subcommand and
# arguments)
#command: 'docker {docker_args}'

# Managed services should be started with this string at the beginning of their
# name. It should match the value of CONTAINER_NAME_PREFIX in
# lib/galaxy/web/base/interactive_environments.py, so you should not change
# this unless you change both.
#service_prefix: galaxy_gie_

# Limits:
#
# - max_waiting_services: number of services that should be waiting of each
#   "CPU class" (number of CPUs requested e.g. with --reserve-cpu) before
#   attempting to spawn a node
# - max_wait_time: number of seconds a service should be waiting before
#   attempting to spawn a node
# - max_node_idle_time: number of seconds a node should be idle before
#   terminating it
# - max_node_counts: a dictionary controlling the maximum number of nodes of
#   each CPU class that the swarm manager will attempt to spawn, e.g.:
#     max_node_counts:
#       1: 10           # spawn up to 10 x 1-CPU nodes
#       2: 3            # spawn up to 3 x 2-CPU nodes
#       4: 1            # spawn up to 1 x 4-CPU nodes
#max_waiting_services: 0
#max_wait_time: 5
#max_node_idle_time: 120
#max_node_counts: {}

# If set, only manage nodes whose swarm hostnames begin with this prefix.
# Otherwise, attempt to manage all nodes
#node_prefix: null

# Amount of time to wait for a spawning node to appear in `docker node ls`
# before considering it failed
#spawn_wait_time: 30

# Command to run to spawn new nodes. This command should join the node to the
# swarm. Can include template variables:
#   - {cpu_class}: CPU class as explained above (the value of --reserve-cpu)
#   - {cpus_needed}: Total number of CPUs of the given class needed to run the
#     waiting services
# If this command does not block until the node is joined to the swarm, make
# sure it at least completes that step in `spawn_wait_time` once it returns
# control. This command should return a space-separated list of nodes. If the
# nodes have a different number of CPUs than the class that they were started
# for, you can include that number after a colon (e.g. `node1:4`).
#spawn_command: /bin/true

# Command to run to destroy idle nodes. Can include template variables:
#   - {nodes}: Space-separated list of node names to destroy
# This command should block until at least the point at which any nodes being
# deallocated no longer appear in `docker node ls`.
#destroy_command: /bin/true

# Command to run if either of the above commands failed (e.g. to notify an
# administrator). Can include template variables:
#   - {failed_command}: Command line of the command that failed
#command_failure_command: /bin/true

# Number of times to retry spawn/destroy commands before considering them to
# have failed, and seconds to wait between retries
#command_retries: 0
#command_retry_wait: 10

# Stop the swarm manager daemon when there are no services or nodes to manage
#terminate_when_idle: True
